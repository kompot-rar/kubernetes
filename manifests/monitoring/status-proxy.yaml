apiVersion: apps/v1
kind: Deployment
metadata:
  name: status-proxy
  namespace: monitoring
  labels:
    app: status-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: status-proxy
  template:
    metadata:
      labels:
        app: status-proxy
    spec:
      containers:
      - name: proxy
        image: node:18-alpine
        workingDir: /app
        command: ["sh", "-c"]
        args:
        - |
          cd /app && \
          npm init -y && \
          npm install --production --no-package-lock --no-audit --no-fund express axios cors && \
          cat <<'EOF' > index.js
          const express = require('express');
          const axios = require('axios');
          const cors = require('cors');
          const app = express();
          app.use(cors());

          const PROM_URL = "http://monitoring-kube-prometheus-prometheus.monitoring.svc:9090/api/v1/query";

          const queryProm = async (q) => {
            try {
              const res = await axios.get(PROM_URL, { params: { query: q }, timeout: 5000 });
              return res.data.data.result;
            } catch (e) {
              console.error(`Query Error (${q}):`, e.message);
              return [];
            }
          };

          app.get('/api/status', async (req, res) => {
            try {
              const [temp, pods, ram, cpu, uptime, restarts24h, unhealthyPods, argoSync, chaosSectors, chaosErrors, chaosWear, chaosWriteLat, chaosInfo, chaosEvents, chaosReportedUncorrect, chaosSmartHealthy, chaosLifeRemain, chaosPowerHours] = await Promise.all([
                queryProm('max(node_hwmon_temp_celsius) by (instance)'),
                queryProm('sum(kube_pod_status_phase{phase="Running"})'),
                queryProm('100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))'),
                queryProm('100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)'),
                queryProm('time() - node_boot_time_seconds'),
                queryProm('sum(increase(kube_pod_container_status_restarts_total[24h]))'),
                queryProm('sum(kube_pod_status_phase{phase=~"Failed|Pending|Unknown"})'),
                queryProm('count(argocd_app_info{sync_status="Synced"})'),
                // Chaos Monkey: Trash Disk Metrics (Crucial MX300 on 10.0.10.13)
                queryProm('smartmon_reallocated_sector_count_raw_value{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('smartmon_offline_uncorrectable_raw_value{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('smartmon_wear_leveling_count_raw_value{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('rate(node_disk_write_time_seconds_total{instance="10.0.10.13:9100", device="sda"}[5m])'),
                queryProm('smartmon_device_info{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('smartmon_reallocated_event_count_raw_value{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('smartmon_reported_uncorrect_raw_value{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('smartmon_device_smart_healthy{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('smartmon_percent_lifetime_remain_raw_value{instance="10.0.10.13:9100", disk="/dev/sda"}'),
                queryProm('smartmon_power_on_hours_raw_value{instance="10.0.10.13:9100", disk="/dev/sda"}')
              ]);

              const nodes = temp.map((t) => {
                const instance = t.metric.instance;
                const cpuVal = cpu.find(c => c.metric.instance === instance);
                const ramVal = ram.find(r => r.metric.instance === instance);
                const upVal = uptime.find(u => u.metric.instance === instance);
                
                return {
                  name: instance.split(':')[0],
                  temp: t.value ? parseFloat(t.value[1]).toFixed(1) : "0",
                  cpu: cpuVal ? parseFloat(cpuVal.value[1]).toFixed(1) : "0",
                  ram: ramVal ? parseFloat(ramVal.value[1]).toFixed(1) : "0",
                  uptime: upVal ? (parseFloat(upVal.value[1]) / 86400).toFixed(1) : "0"
                };
              });

              // Chaos Monkey Logic
              const sectors = chaosSectors[0] ? parseInt(chaosSectors[0].value[1]) : 0;
              const errors = chaosErrors[0] ? parseInt(chaosErrors[0].value[1]) : 0;
              const wear = chaosWear[0] ? parseInt(chaosWear[0].value[1]) : 0;
              const writeLat = chaosWriteLat[0] ? parseFloat(chaosWriteLat[0].value[1]) : 0;
              const info = chaosInfo[0] ? chaosInfo[0].metric : {};
              const events = chaosEvents[0] ? parseInt(chaosEvents[0].value[1]) : 0;
              const reportedUncorrect = chaosReportedUncorrect[0] ? parseInt(chaosReportedUncorrect[0].value[1]) : 0;
              const isSmartHealthy = chaosSmartHealthy[0] ? parseInt(chaosSmartHealthy[0].value[1]) === 1 : false;
              const lifeRemain = chaosLifeRemain[0] ? parseInt(chaosLifeRemain[0].value[1]) : 100;
              const powerHours = chaosPowerHours[0] ? parseInt(chaosPowerHours[0].value[1]) : 0;

              let chaosStatus = "Stable (For a dumpster disk)";
              let chaosNote = "Running on salvaged hardware. No data is safe here.";
              
              if (reportedUncorrect > 1000) {
                chaosStatus = "Radioactive (ECC Failure)";
                chaosNote = "Massive hardware ECC errors. Bit-rot is actively occurring.";
              } else if (lifeRemain < 10) {
                chaosStatus = "EOL (End of Life)";
                chaosNote = "NAND cells exhausted. Write operations are high-risk.";
              } else if (errors > 0) {
                chaosStatus = "Dying (Data Loss Detected)";
                chaosNote = "Uncorrectable sectors found. SMART self-test is screaming.";
              } else if (writeLat > 0.5) {
                chaosStatus = "Throttling (Floppy Disk Mode)";
                chaosNote = "High write latency detected. Controller struggling with bad cells.";
              }

              const restartCount = restarts24h[0] ? Math.round(parseFloat(restarts24h[0].value[1])) : 0;
              const unhealthyCount = unhealthyPods[0] ? parseInt(unhealthyPods[0].value[1]) : 0;
              const isSynced = argoSync[0] ? parseInt(argoSync[0].value[1]) > 0 : false;
              
              let clusterStatus = "Healthy";
              let statusMessage = isSynced ? "GitOps Synced. All systems operational." : "Cluster Operational. Manual changes detected.";
              
              if (unhealthyCount > 0) {
                clusterStatus = "Warning";
                statusMessage = `Self-healing active: ${unhealthyCount} pod(s) recovering.`;
              }

              res.json({
                cluster: {
                  totalPods: pods[0] ? pods[0].value[1] : "0",
                  status: clusterStatus,
                  message: statusMessage,
                  gitops: isSynced ? "Synced" : "Out of Sync",
                  lastUpdate: new Date().toISOString()
                },
                chaos_monkey_audit: {
                  target: info.device_model || "Crucial_MX300 (Dumpster)",
                  serial: info.serial_number || "N/A",
                  node: "10.0.10.13",
                  health_status: chaosStatus,
                  firmware_verdict: isSmartHealthy ? "PASSED (Optimistic)" : "FAILED (Critical)",
                  alert_level: reportedUncorrect > 0 ? "Radioactive" : (lifeRemain < 30 ? "Critical" : (errors > 0 ? "Critical" : (sectors > 0 || events > 0 ? "Warning" : "Nominal"))),
                  metrics: {
                    reallocated_sectors: sectors,
                    reallocated_events: events,
                    offline_uncorrectable: errors,
                    reported_uncorrectable_ecc: reportedUncorrect,
                    ssd_wear_cycles: wear,
                    lifetime_remain_pct: lifeRemain,
                    power_on_hours: powerHours,
                    avg_write_lat_sec: writeLat.toFixed(4)
                  },
                  sre_message: chaosNote
                },
                nodes: nodes
              });
            } catch (e) {
              res.status(502).json({ error: "Błąd serwera statusu", details: e.message });
            }
          });

          app.get('/health', (req, res) => res.send('OK'));
          app.listen(3000, () => console.log('Super-Status Proxy v2.2 (24h Window) listening on port 3000'));
          EOF
          node index.js
        ports:
        - containerPort: 3000
        resources:
          requests:
            cpu: 20m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  name: status-proxy
  namespace: monitoring
spec:
  selector:
    app: status-proxy
  ports:
  - port: 80
    targetPort: 3000
